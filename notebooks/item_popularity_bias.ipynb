{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook #2: Investigation on Item Popularity\n",
    "\n",
    "Recommender systems are **bridging users and relevant products**, services and peers on the Web. By leveraging past behavioural data, such automated systems aim to learn patterns behind users’ preferences and predict the future interests of users. Notable examples are integrated into platforms that cover different contexts, including e-commerce (e.g., Amazon, eBay), entertainment (e.g., YouTube, Netflix), and education (e.g., edX, Udemy). The success of these platforms strongly depends on the effectiveness of their recommender system.\n",
    "\n",
    "The increasing adoption of recommender systems in online platforms has spurred investigations on **issues of bias** in their internal mechanisms. One aspect that has received attention is the recommender systems’ tendency of emphasizing a “rich-get-richer” effect in favor of **popular items**. Such a phenomenon leads to a loop where recommender systems trained on data non-uniformly distributed across items tend to suggest popular items more than niche items, even when the latter would be of interest. Thus, **popular items gain higher visibility and become more likely to be selected**. The awareness of this type of bias might even lead providers to bribe users, so that they rate or increase the ratings given to their items, thus allowing these items to get more visibility. Under this repeated loop, **the training data becomes imbalanced towards a tiny set of items** more and more, as a result of a strong popularity bias.\n",
    "\n",
    "Recommender systems that **suggest the most popular items have been proved to achieve competitive accuracy**, compared with advanced recommendation techniques. However, the literature has acknowledged that other aspects beyond the accuracy of the recommender system, such as whether the suggested items are novel and cover well the catalog, can make a positive impact on the overall recommendation quality. Unfortunately, **a bias against item popularity may emphasize the occurrence of filter bubbles**, thus hampering users’ interest and several beyond-accuracy aspects. Trading such aspects for item popularity might likely not be accepted by the users who finally receive the recommendations. Therefore, mitigating the impact of a popularity bias can help to meet **a better trade-off between accuracy and beyond-accuracy objectives**, with a clear benefit for the recommendation quality.\n",
    "\n",
    "Existing frameworks and techniques for mitigating a popularity bias are often based on evaluation metrics that do not account for users’ preferences, being assessed only on the level of popularity of items in a recommended list. However, **controlling popularity cannot be a final objective concept**, as it strongly depends on users’ preferences and on how data has been collected. It follows that popularity metrics and bias mitigation need to account for the users’ tastes and the visibility given to items thanks to recommendations, creating a bridge between these perspectives by means of beyond-accuracy objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the working environment for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/drive/My Drive/icdm2020/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git fetch --all\n",
    "! git reset --hard origin/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/drive/My Drive/icdm2020/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.utils import *\n",
    "from helpers.train_test_splitter import *\n",
    "from models.pairwise_reg import PairWise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will load the **Movielens 1M** dataset, which has been pre-arranged in order to comply with the following structure: user_id, item_id, rating, timestamp, type (label for the item category), and type_id (unique id of the item category). For the sake of tutorial easiness, we assume here that each item is randomly assigned to one of its categories in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'ml1m'\n",
    "smode = 'utime'\n",
    "user_field = 'user_id'\n",
    "item_field = 'item_id'\n",
    "rating_field = 'rating'\n",
    "time_field = 'timestamp'\n",
    "type_field = 'type_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  pd.read_csv('../data/outputs/splits/' + dataset + '_' + smode + '.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the number of ratings is a proxy of the popularity of the item. We calculate the popularity of each item and we sort items in a dataframe by decreasing popularity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_pop = data.groupby([item_field]).count().sort_values(user_field, ascending=False)[user_field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of easiness, we conduct our study focusing on two sets of items: most popular items and less popular items. To this end, we will consider the most popular 800 items in the first set, while the remaining items are included in the second set. Several ways to split items in these two sets arefound in literature (e.g., most popular items that receive the 80% of the overall ratings).    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_tail_split = 808 # Magic number\n",
    "head_tail_items = np.array(item_pop[:head_tail_split].index)\n",
    "long_tail_items = np.array(item_pop[head_tail_split:].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Head Tail', compute_gini(item_pop[:head_tail_split]))\n",
    "print('Long Tail', compute_gini(item_pop[head_tail_split:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 8})\n",
    "plt.figure(figsize=(6, 3.5))\n",
    "\n",
    "plt.title(r'Item popularity')\n",
    "plt.xlabel('Items ranked by popularity')\n",
    "plt.ylabel('Number of ratings in the dataset')\n",
    "plt.plot(range(head_tail_split), item_pop.values[:head_tail_split], alpha=0.7, label=r'Head tail')\n",
    "plt.plot(range(head_tail_split, len(item_pop.index)), item_pop.values[head_tail_split:], label=r'Long tail')\n",
    "plt.axhline(y=item_pop.values[head_tail_split], linestyle='--', lw=1, c='grey')\n",
    "plt.axvline(x=head_tail_split, linestyle='--', lw=1, c='grey')\n",
    "plt.xlim([-25, len(item_pop.index)])\n",
    "plt.ylim([-25, item_pop.values[0]])\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note how the curve is skewed towards items with only few ratings. In our case, most popular items are those receving more than 450 ratings. In the rest of this study, we will investigate how recommendation algorithms treat items belonging to these sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Analysis of the popularity of the recommended items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same cutoffs we have configured in the first notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = np.array([5, 10, 20, 50, 100, 200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will first compare the characteristics of the items recommended by pairwise, pointwise, random and mostpop strategies. Later on, we will come back here to compare also the results of the mitigation procedures with those of the above recommendation algorithms.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = ['utime_pointwise', 'utime_pairwise', 'utime_random', 'utime_mostpop']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up, we will load the metrics pre-computed in the first notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metrics(model_types):\n",
    "    metrics = {}\n",
    "    for model_type in model_types:\n",
    "        metrics[model_type] = load_obj(os.path.join(data_path, 'outputs/metrics/'+dataset+'_'+model_type+'_metrics.pkl'))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = load_metrics(model_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will compare the considered algorithms based on their recommendation effectiveness: precision, recall, and NDCG. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_effectiveness(metrics, cutoffs):\n",
    "    plt.rcParams.update({'font.size': 16.5})\n",
    "    plt.figure(figsize=(30, 7.5))\n",
    "\n",
    "    plt.subplot(131)\n",
    "    plt.title(r'Precision')\n",
    "    plt.xlabel('Cutoff Value')\n",
    "    plt.ylabel('Precision')\n",
    "    for model_type in model_types:\n",
    "        plt.plot(cutoffs, [np.mean(metrics[model_type]['precision'][k,:]) for k in range(len(cutoffs))], label=model_type)\n",
    "    plt.xticks(cutoffs)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y')\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.title(r'Recall')\n",
    "    plt.xlabel('Cutoff Value')\n",
    "    plt.ylabel('Recall')\n",
    "    for model_type in model_types:\n",
    "        plt.plot(cutoffs, [np.mean(metrics[model_type]['recall'][k,:]) for k in range(len(cutoffs))], label=model_type)\n",
    "    plt.xticks(cutoffs)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y')\n",
    "\n",
    "    plt.subplot(133)\n",
    "    plt.title(r'NDCG')\n",
    "    plt.xlabel('Cutoff Value')\n",
    "    plt.ylabel('NDCG')\n",
    "    for model_type in model_types:\n",
    "        plt.plot(cutoffs, [np.mean(metrics[model_type]['ndcg'][k,:]) for k in range(len(cutoffs))], label=model_type)\n",
    "    plt.xticks(cutoffs)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y')\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_effectiveness(metrics, cutoffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that pairwise and mostpop algorithms have a really similar behavior for all the considered metrics. Nove, we move our attention to the popularity of the recommended items and the coverage of the items we marked as \"less popular\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_item_related_metrics(metrics, cutoffs):\n",
    "    plt.rcParams.update({'font.size': 16.5})\n",
    "    plt.figure(figsize=(30, 7.5))\n",
    "\n",
    "    plt.subplot(131)\n",
    "    plt.title(r'Average Popularity of Items')\n",
    "    plt.xlabel('Cutoff Value')\n",
    "    plt.ylabel('APT')\n",
    "    for model_type in model_types:\n",
    "        plt.plot(cutoffs, [np.mean(metrics[model_type]['mean_popularity'][k,:]) for k in range(len(cutoffs))], label=model_type)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y')\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.title(r'Average Percentage of Less Popular Items')\n",
    "    plt.xlabel('Cutoff Value')\n",
    "    plt.ylabel('APLT')\n",
    "    for model_type in model_types:\n",
    "        plt.plot(cutoffs, [np.sum(metrics[model_type]['item_coverage'][k,long_tail_items]) / np.sum(metrics[model_type]['item_coverage'][k,:]) for k in range(len(cutoffs))], label=model_type)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y')\n",
    "\n",
    "    plt.subplot(133)\n",
    "    plt.title(r'Item Coverage')\n",
    "    plt.xlabel('Cutoff Value')\n",
    "    plt.ylabel('Item Coverage')\n",
    "    for i, model_type in enumerate(model_types):\n",
    "        plt.plot(cutoffs, [len([1 for m in metrics[model_type]['item_coverage'][k,head_tail_items] if m > 0]) / len(head_tail_items) for k in range(len(cutoffs))], color='C'+str(i), linestyle='-', label='pop head of ' + model_type)\n",
    "        plt.plot(cutoffs, [len([1 for m in metrics[model_type]['item_coverage'][k,long_tail_items] if m > 0]) / len(long_tail_items) for k in range(len(cutoffs))], color='C'+str(i), linestyle=':', label='pop tail of ' + model_type)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y')\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_item_related_metrics(metrics, cutoffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These figures show us that the average popularity of items recommended by the pairwise algorithm is really high and not so far from the one of items recommended by mostpop. This observation is also confirmed by the coverage of items from the \"less popular\" set. For small cutoffs, pairwise and mostpop recommened only a tiny fraction of the less popular items. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Promote less popular items via in-processing\n",
    "\n",
    "We investigate how a recommender system can generate recommendations less biased against popularity. To this end, we propose a mitigation procedure that aims at minimizing both (i) the loss function targeted by the considered recommendation algorithm, and (ii) the biased correlation between the predicted relevance and the popularity of the observed item. We show a didactic version of the correlation-based regularization proposed by Boratto et al. (2021).\n",
    "\n",
    "- Boratto, Ludovico, Gianni Fenu, and Mirko Marras. Connecting user and item perspectives in popularity debiasing for collaborative recommendation, Information Processing & Management, Volume 58, Issue 1, 2021, 102387, ISSN 0306-4573, https://doi.org/10.1016/j.ipm.2020.102387."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run again the recommendation pipeline, starting by the train-test split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smode = 'utime'\n",
    "train_ratio = 0.80        \n",
    "min_train_samples = 8\n",
    "min_test_samples = 2\n",
    "min_time = None\n",
    "max_time = None\n",
    "step_time = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if smode == 'uftime':\n",
    "    traintest = fixed_timestamp(data, min_train_samples, min_test_samples, min_time, max_time, step_time, user_field, item_field, time_field, rating_field)\n",
    "elif smode == 'utime':\n",
    "    traintest = user_timestamp(data, train_ratio, min_train_samples+min_test_samples, user_field, item_field, time_field)\n",
    "elif smode == 'urandom':\n",
    "    traintest = user_random(data, train_ratio, min_train_samples+min_test_samples, user_field, item_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = traintest[traintest['set']=='train'].copy()\n",
    "test = traintest[traintest['set']=='test'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(np.unique(traintest[user_field].values))\n",
    "items = list(np.unique(traintest[item_field].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_metadata = traintest.drop_duplicates(subset=['item_id'], keep='first')\n",
    "category_per_item = items_metadata[type_field].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_item_association = pd.read_csv(os.path.join(data_path, 'datasets', 'ml1m-dir-gender.csv')) \n",
    "gender_maps = {i:g for i, g in zip(gender_item_association['item_id'], gender_item_association['gender_1'])}\n",
    "item_maps = {i1:i2 for i1, i2 in zip(traintest['item_id'].unique(), traintest['item_id_original'].unique())}\n",
    "item_group = [(1 if item_maps[i] in gender_maps and gender_maps[item_maps[i]] == 0 else 0) for i in range(len(items))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a supporting function to run a regularized version of the pairwise recommender model, where *rweight* represents the weights given to the regularization during the learning phase. The higher the weight is, the more the correlation between popularity and predicted margin is reduced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_reg_pairwise_model(rweight=0.0, no_epochs=None):\n",
    "    model_id = smode + '_' + str(int(rweight*100)) + '_' + 'pairwise'\n",
    "    \n",
    "    print('\\n***Running experimental pipeline for model with id', model_id)\n",
    "    \n",
    "    model = PairWise(users, items, train, test, category_per_item, item_field, user_field, rating_field)\n",
    "    model.train(no_epochs=no_epochs, rweight=rweight) if no_epochs else model.train(rweight=rweight) \n",
    "    model.predict()\n",
    "    scores = model.get_predictions()\n",
    "    save_obj(scores, os.path.join(data_path, 'outputs/predictions/' + dataset + '_' + model_id + '_scores.pkl'))\n",
    "    \n",
    "    model.test(item_group=item_group, cutoffs=cutoffs)\n",
    "    \n",
    "    metrics = model.get_metrics()\n",
    "    save_obj(metrics, os.path.join(data_path, 'outputs/metrics/' + dataset + '_' + model_id + '_metrics.pkl'))\n",
    "    \n",
    "    print('\\nFinal evaluation metrics:')\n",
    "    model.show_metrics(index_k=int(np.where(cutoffs == 10)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two trials with the new regularization. The results will be then compared against the ones achieved by the base pairwise recommender model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rweights = [0.01, 0.05] \n",
    "\n",
    "for r in rweights:\n",
    "    run_reg_pairwise_model(rweight=r, no_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the results and compare them against each other, we load the pre-computed metrics and use the supporting plot functions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = ['utime_pairwise', 'utime_1_pairwise', 'utime_5_pairwise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = load_metrics(model_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_effectiveness(metrics, cutoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_item_related_metrics(metrics, cutoffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Promote less popular items via post-processing\n",
    "\n",
    "In this part, we will show how to setup and perform a post-processing mitigation approach. We show a didactic version of the xQuad algorithm adaptation proposed by Adbollahpouri et al. (2018). Intuitively, they provide a function with two terms: the first term incorporates ranking accuracy while the second term promotes diversity between two different categories of items (i.e. short head and long tail). A weight controls how strongly controlling popularity bias is.\n",
    "\n",
    "- Himan Abdollahpouri, Robin Burke, Bamshad Mobasher: Managing Popularity Bias in Recommender Systems with Personalized Re-Ranking. FLAIRS Conference 2019: 413-418. https://aaai.org/ocs/index.php/FLAIRS/FLAIRS19/paper/view/18199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traintest = pd.read_csv('../data/outputs/splits/' + dataset + '_' + smode + '.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = traintest[traintest['set']=='train'].copy()\n",
    "test = traintest[traintest['set']=='test'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(np.unique(traintest[user_field].values))\n",
    "items = list(np.unique(traintest[item_field].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_metadata = traintest.drop_duplicates(subset=['item_id'], keep='first')\n",
    "category_per_item = items_metadata[type_field].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to import and usethe re-ranking model mentioned above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ranker_xquad import RankerXQuad\n",
    "model = RankerXQuad(users, items, train, test, category_per_item, item_field, user_field, rating_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model_type = 'utime_pairwise' # This string identifies the recommendation algorithm where the re-ranking is applied\n",
    "reranked_model_type = original_model_type + '_' + 'xquad' # This string is an identifier for models results in data/outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the model predictions with those of the original recommendation model, precomputed in the first notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = load_obj(os.path.join(data_path,'outputs/predictions/' + dataset + '_' + original_model_type + '_scores.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_predictions(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run the re-ranking process and save the positional relevance of items for users after re-ranking.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rerank(type='smooth', lmbda=0.4, k=10, rmax=100, head_tail_split=head_tail_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.get_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(predictions, os.path.join(data_path, 'outputs/predictions/' + dataset + '_' + reranked_model_type + '_scores.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute and show the metrics for the recommender systems obtained after re-ranking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test(cutoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(metrics, os.path.join(data_path, 'outputs/metrics/' + dataset + '_' + reranked_model_type + '_metrics.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To speed up of process in this tutorial\n",
    "\n",
    "This re-ranking takes several minutes. For this tutorial, please feel free to stop and load directly our pre-computed re-ranking predictions (2 MB).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyDrive\n",
    "\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "downloaded = drive.CreateFile({'id':'1lAmSPd19F4WRfWz0TOdFcOzHUMieSJOX'}) \n",
    "downloaded.GetContentFile('../data/outputs/metrics/ml1m_utime_pairwise_xquad_metrics.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_metrics(load_obj('../data/outputs/metrics/ml1m_utime_pairwise_xquad_metrics.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.show_metrics(index_k=int(np.where(cutoffs == 10)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have obtained the metrics resulting from the considered strategy. Now, we come back to compare the results obtained with these strategy against the ones of the baseline recommendation algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = ['utime_pointwise', 'utime_pairwise', 'utime_pairwise_xquad', 'utime_random', 'utime_mostpop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = load_metrics(model_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_effectiveness(metrics, np.array([5, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_item_related_metrics(metrics, np.array([5, 10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow-up: how to extend the toolbox with mitigation strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Strategy at dataset level: just apply your mitigation strategy to the original csv of the dataset and swap it in our code. \n",
    "- Strategy at train instance level: just create another generator in helpers/instances_creator.py and use it in our code.\n",
    "- Strategy at learning level: create a copy of the model you want to consider for your strategy (e.g., models/pairwise.py) and add your mitigation strategy. \n",
    "- Strategy at ranking level: create a copy of the ranker mentioned above (i.e., models/ranker_xquad) and use it as a basis for another re-ranking strategy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
