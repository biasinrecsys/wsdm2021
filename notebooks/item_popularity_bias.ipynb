{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook #2: Investigation on Item Popularity\n",
    "\n",
    "\n",
    "**Context**. Recommender systems are bridging users and relevant products, services and peers on the Web. By leveraging past behavioural data, such automated systems aim to learn patterns behind users’ preferences and predict the future interests of users. Notable examples are integrated into platforms that cover different contexts, including e-commerce (e.g., Amazon, eBay), entertainment (e.g., YouTube, Netflix), and education (e.g., edX, Udemy). The success of these platforms strongly depends on the effectiveness of their recommender system.\n",
    "\n",
    "**Motivation**. The increasing adoption of recommender systems in online platforms has spurred investigations on issues of bias in their internal mechanisms. One aspect that has received attention is the recommender systems’ tendency of emphasizing a “rich-get-richer” effect in favor of popular items. Such a phenomenon leads to a loop where recommender systems trained on data non-uniformly distributed across items tend to suggest popular items more than niche items, even when the latter would be of interest. Thus, popular items gain higher visibility and become more likely to be selected. The awareness of this type of bias might even lead providers to bribe users, so that they rate or increase the ratings given to their items, thus allowing these items to get more visibility. Under this repeated loop, the training data becomes imbalanced towards a tiny set of items more and more, as a result of a strong popularity bias.\n",
    "\n",
    "**Problem Statement**. Recommender systems that suggest the most popular items have been proved to achieve competitive accuracy, compared with advanced recommendation techniques. However, the literature has acknowledged that other aspects beyond the accuracy of the recommender system, such as whether the suggested items are novel and cover well the catalog, can make a positive impact on the overall recommendation quality. Unfortunately, a bias against item popularity may emphasize the occurrence of filter bubbles, thus hampering users’ interest and several beyond-accuracy aspects. Trading such aspects for item popularity might likely not be accepted by the users who finally receive the recommendations. Therefore, mitigating the impact of a popularity bias can help to meet a better trade-off between accuracy and beyond-accuracy objectives, with a clear benefit for the recommendation quality.\n",
    "\n",
    "Existing frameworks and techniques for mitigating a popularity bias are often based on evaluation metrics that do not account for users’ preferences, being assessed only on the level of popularity of items in a recommended list. However, controlling popularity cannot be a final objective concept, as it strongly depends on users’ preferences and on how data has been collected. It follows that popularity metrics and bias mitigation need to account for the users’ tastes and the visibility given to items thanks to recommendations, creating a bridge between these perspectives by means of beyond-accuracy objectives.\n",
    "\n",
    "**Hands-on Outline**. In this notebook, we will focus on analyzing and mitigating biases against item popularity through our introductory Python toolbox, in the simplest possible way. Specifically, we will:\n",
    "\n",
    "- **Step 1** Load the working environment from GDrive. \n",
    "\n",
    "- **Step 2** Load the Movielens 1M dataset.\n",
    "\n",
    "- **Step 3** Inspect the recommended lists based on item popularity.\n",
    "\n",
    "- **Step 4** Define, run, and inspect the impact of an in-processing mitigation.\n",
    "\n",
    "- **Step 5** Define, run, and inspect the impact of a post-processing mitigation.\n",
    "\n",
    "- **Step 6**: Compare the impact of two different mitigation procedures. \n",
    "\n",
    "For this notebook, we need to have done the computations (e.g., pre-trained models, user-item relevance matrices and so on) for the algorithms showed in the first hands-on activity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Load the working environment from GDrive. \n",
    "\n",
    "This step serves to mount GDrive storage within this Jupyter notebook. The command will request us to give access permissions to this notebook, so that we will be able to clone the project repository when we desire. Please follow the prompted instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We move to the folder with the notebook in our tutorial codebase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/drive/My Drive/wsdm2021/notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "sys.path.append(os.path.join('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.utils import *\n",
    "from helpers.train_test_splitter import *\n",
    "from models.pairwise_reg import PairWise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the base path to the pre-computed artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the Movielens 1M dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will load the **Movielens 1M** dataset, which has been pre-arranged in order to comply with the following structure: user_id, item_id, rating, timestamp, type (label for the item category), and type_id (unique id of the item category). For the sake of tutorial easiness, we assume here that each item is randomly assigned to one of its categories in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'ml1m'\n",
    "smode = 'utime'\n",
    "user_field = 'user_id'\n",
    "item_field = 'item_id'\n",
    "rating_field = 'rating'\n",
    "time_field = 'timestamp'\n",
    "type_field = 'type_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  pd.read_csv('../data/outputs/splits/' + dataset + '_' + smode + '.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the number of ratings is a proxy of the popularity of the item. We calculate the popularity of each item and we sort items in a dataframe by decreasing popularity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_pop = data.groupby([item_field]).count().sort_values(user_field, ascending=False)[user_field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of easiness, we conduct our study focusing on two sets of items: most popular items and less popular items. To this end, we will consider the most popular 800 items in the first set, while the remaining items are included in the second set. Several ways to split items in these two sets arefound in literature (e.g., most popular items that receive the 80% of the overall ratings).    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_tail_split = 808 # Magic number for our tutorial showcase\n",
    "head_tail_items = np.array(item_pop[:head_tail_split].index)\n",
    "long_tail_items = np.array(item_pop[head_tail_split:].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then measure the inequality in popularity for the items in the two sets. \n",
    "\n",
    "To this end, we use the **Gini coefficient**, a measure of inequality among values of a frequency distribution. \n",
    "\n",
    "- A Gini coefficient of zero expresses perfect equality, where all values are the same. \n",
    "- A Gini coefficient of one expresses maximal inequality among values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Head Tail', compute_gini(item_pop[:head_tail_split]))\n",
    "print('Long Tail', compute_gini(item_pop[head_tail_split:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the difference in popularity visually, we plot the distirbution of the number of ratings received by each item. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 8})\n",
    "plt.figure(figsize=(6, 3.5))\n",
    "\n",
    "plt.title(r'Item popularity')\n",
    "plt.xlabel('Items ranked by popularity')\n",
    "plt.ylabel('Number of ratings in the dataset')\n",
    "plt.plot(range(head_tail_split), item_pop.values[:head_tail_split], alpha=0.7, label=r'Head tail')\n",
    "plt.plot(range(head_tail_split, len(item_pop.index)), item_pop.values[head_tail_split:], label=r'Long tail')\n",
    "plt.axhline(y=item_pop.values[head_tail_split], linestyle='--', lw=1, c='grey')\n",
    "plt.axvline(x=head_tail_split, linestyle='--', lw=1, c='grey')\n",
    "plt.xlim([-25, len(item_pop.index)])\n",
    "plt.ylim([-25, item_pop.values[0]])\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note how the curve is skewed towards items with only few ratings. In our case, most popular items are those receving more than 808 ratings. In the rest of this study, we will investigate how recommendation algorithms treat items belonging to these sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Inspect the recommended lists based on item popularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same cutoffs we have configured in the first notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = np.array([5, 10, 20, 50, 100, 200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will first compare the characteristics of the items recommended by pairwise, pointwise, random and mostpop strategies. Later on, we will come back here to compare also the results of the mitigation procedures with those of the above recommendation algorithms.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = ['utime_pointwise', 'utime_pairwise', 'utime_random', 'utime_mostpop']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up, we will load the metrics pre-computed in the first notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metrics(model_types):\n",
    "    metrics = {}\n",
    "    for model_type in model_types:\n",
    "        metrics[model_type] = load_obj(os.path.join(data_path, 'outputs/metrics/'+dataset+'_'+model_type+'_metrics.pkl'))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = load_metrics(model_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will compare the considered algorithms based on their recommendation effectiveness: precision, recall, and NDCG. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_effectiveness(metrics, cutoffs):\n",
    "    plt.rcParams.update({'font.size': 16.5})\n",
    "    plt.figure(figsize=(30, 7.5))\n",
    "\n",
    "    plt.subplot(131)\n",
    "    plt.title(r'Precision')\n",
    "    plt.xlabel('Cutoff Value')\n",
    "    plt.ylabel('Precision')\n",
    "    for model_type in model_types:\n",
    "        plt.plot(cutoffs, [np.mean(metrics[model_type]['precision'][k,:]) for k in range(len(cutoffs))], label=model_type)\n",
    "    plt.xticks(cutoffs)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y')\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.title(r'Recall')\n",
    "    plt.xlabel('Cutoff Value')\n",
    "    plt.ylabel('Recall')\n",
    "    for model_type in model_types:\n",
    "        plt.plot(cutoffs, [np.mean(metrics[model_type]['recall'][k,:]) for k in range(len(cutoffs))], label=model_type)\n",
    "    plt.xticks(cutoffs)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y')\n",
    "\n",
    "    plt.subplot(133)\n",
    "    plt.title(r'NDCG')\n",
    "    plt.xlabel('Cutoff Value')\n",
    "    plt.ylabel('NDCG')\n",
    "    for model_type in model_types:\n",
    "        plt.plot(cutoffs, [np.mean(metrics[model_type]['ndcg'][k,:]) for k in range(len(cutoffs))], label=model_type)\n",
    "    plt.xticks(cutoffs)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y')\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_effectiveness(metrics, cutoffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that pairwise and mostpop algorithms have a really similar behavior for all the considered metrics. Nove, we move our attention to the popularity of the recommended items and the coverage of the items we marked as \"less popular\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_item_related_metrics(metrics, cutoffs):\n",
    "    plt.rcParams.update({'font.size': 16.5})\n",
    "    plt.figure(figsize=(30, 7.5))\n",
    "\n",
    "    plt.subplot(131)\n",
    "    plt.title(r'Average Popularity of Items')\n",
    "    plt.xlabel('Cutoff Value')\n",
    "    plt.ylabel('APT')\n",
    "    for model_type in model_types:\n",
    "        plt.plot(cutoffs, [np.mean(metrics[model_type]['mean_popularity'][k,:]) for k in range(len(cutoffs))], label=model_type)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y')\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.title(r'Average Percentage of Less Popular Items')\n",
    "    plt.xlabel('Cutoff Value')\n",
    "    plt.ylabel('APLT')\n",
    "    for model_type in model_types:\n",
    "        plt.plot(cutoffs, [np.sum(metrics[model_type]['item_coverage'][k,long_tail_items]) / np.sum(metrics[model_type]['item_coverage'][k,:]) for k in range(len(cutoffs))], label=model_type)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y')\n",
    "\n",
    "    plt.subplot(133)\n",
    "    plt.title(r'Item Coverage')\n",
    "    plt.xlabel('Cutoff Value')\n",
    "    plt.ylabel('Item Coverage')\n",
    "    for i, model_type in enumerate(model_types):\n",
    "        plt.plot(cutoffs, [len([1 for m in metrics[model_type]['item_coverage'][k,head_tail_items] if m > 0]) / len(head_tail_items) for k in range(len(cutoffs))], color='C'+str(i), linestyle='-', label='pop head of ' + model_type)\n",
    "        plt.plot(cutoffs, [len([1 for m in metrics[model_type]['item_coverage'][k,long_tail_items] if m > 0]) / len(long_tail_items) for k in range(len(cutoffs))], color='C'+str(i), linestyle=':', label='pop tail of ' + model_type)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y')\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_item_related_metrics(metrics, cutoffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These figures show us that the average popularity of items recommended by the pairwise algorithm is really high and not so far from the one of items recommended by mostpop. This observation is also confirmed by the coverage of items from the \"less popular\" set. For small cutoffs, pairwise and mostpop recommened only a tiny fraction of the less popular items. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define, run, and inspect the impact of an in-processing mitigation.\n",
    "\n",
    "We investigate how a recommender system can generate recommendations less biased against popularity. To this end, we propose a mitigation procedure that aims at minimizing both (i) the loss function targeted by the considered recommendation algorithm, and (ii) the biased correlation between the predicted relevance and the popularity of the observed item. We show a didactic version of the correlation-based regularization proposed by Boratto et al. (2021).\n",
    "\n",
    "- Boratto, Ludovico, Gianni Fenu, and Mirko Marras. Connecting user and item perspectives in popularity debiasing for collaborative recommendation, Information Processing & Management, Volume 58, Issue 1, 2021, 102387, ISSN 0306-4573. [Elsevier Link](https://doi.org/10.1016/j.ipm.2020.102387)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run again the recommendation pipeline, starting by the train-test split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smode = 'utime'\n",
    "train_ratio = 0.80        \n",
    "min_train_samples = 8\n",
    "min_test_samples = 2\n",
    "min_time = None\n",
    "max_time = None\n",
    "step_time = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if smode == 'uftime':\n",
    "    traintest = fixed_timestamp(data, min_train_samples, min_test_samples, min_time, max_time, step_time, user_field, item_field, time_field, rating_field)\n",
    "elif smode == 'utime':\n",
    "    traintest = user_timestamp(data, train_ratio, min_train_samples+min_test_samples, user_field, item_field, time_field)\n",
    "elif smode == 'urandom':\n",
    "    traintest = user_random(data, train_ratio, min_train_samples+min_test_samples, user_field, item_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we separate the training set from the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = traintest[traintest['set']=='train'].copy()\n",
    "test = traintest[traintest['set']=='test'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep trace of the user ids and the item ids and other information related to the item metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(np.unique(traintest[user_field].values))\n",
    "items = list(np.unique(traintest[item_field].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_metadata = traintest.drop_duplicates(subset=['item_id'], keep='first')\n",
    "category_per_item = items_metadata[type_field].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_item_association = pd.read_csv(os.path.join(data_path, 'datasets', 'ml1m-dir-group.csv')) \n",
    "group_maps = {i:g for i, g in zip(group_item_association['item_id'], group_item_association['group_1'])}\n",
    "item_maps = {i1:i2 for i1, i2 in zip(traintest['item_id'].unique(), traintest['item_id_original'].unique())}\n",
    "item_group = [(1 if item_maps[i] in group_maps and group_maps[item_maps[i]] == 0 else 0) for i in range(len(items))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a supporting function to run a regularized version of the pairwise recommender model, where *rweight* represents the weights given to the regularization during the learning phase. The higher the weight is, the more the correlation between popularity and predicted margin is reduced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_reg_pairwise_model(rweight=0.0, no_epochs=None):\n",
    "    model_id = smode + '_' + str(int(rweight*100)) + '_' + 'pairwise'\n",
    "    \n",
    "    print('\\n***Running experimental pipeline for model with id', model_id)\n",
    "    \n",
    "    model = PairWise(users, items, train, test, category_per_item, item_field, user_field, rating_field)\n",
    "    model.train(no_epochs=no_epochs, rweight=rweight) if no_epochs else model.train(rweight=rweight) \n",
    "    model.predict()\n",
    "    scores = model.get_predictions()\n",
    "    save_obj(scores, os.path.join(data_path, 'outputs/predictions/' + dataset + '_' + model_id + '_scores.pkl'))\n",
    "    \n",
    "    model.test(item_group=item_group, cutoffs=cutoffs)\n",
    "    \n",
    "    metrics = model.get_metrics()\n",
    "    save_obj(metrics, os.path.join(data_path, 'outputs/metrics/' + dataset + '_' + model_id + '_metrics.pkl'))\n",
    "    \n",
    "    print('\\nFinal evaluation metrics:')\n",
    "    model.show_metrics(index_k=int(np.where(cutoffs == 10)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two trials with the new regularization. The results will be then compared against the ones achieved by the base pairwise recommender model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rweights = [0.01, 0.05] \n",
    "\n",
    "for r in rweights:\n",
    "    run_reg_pairwise_model(rweight=r, no_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the results and compare them against each other, we load the pre-computed metrics and use the supporting plot functions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = ['utime_pairwise', 'utime_1_pairwise', 'utime_5_pairwise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = load_metrics(model_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_effectiveness(metrics, cutoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_item_related_metrics(metrics, cutoffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define, run, and inspect the impact of a post-processing mitigation.\n",
    "\n",
    "In this part, we will show how to setup and perform a post-processing mitigation approach. We show a didactic version of the xQuad algorithm adaptation proposed by Adbollahpouri et al. (2018). Intuitively, they provide a function with two terms: the first term incorporates ranking accuracy while the second term promotes diversity between two different categories of items (i.e. short head and long tail). A weight controls how strongly controlling popularity bias is.\n",
    "\n",
    "- Himan Abdollahpouri, Robin Burke, Bamshad Mobasher: Managing Popularity Bias in Recommender Systems with Personalized Re-Ranking. FLAIRS Conference 2019: 413-418. [AAAI Link](https://aaai.org/ocs/index.php/FLAIRS/FLAIRS19/paper/view/18199)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-load the same training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traintest = pd.read_csv('../data/outputs/splits/' + dataset + '_' + smode + '.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = traintest[traintest['set']=='train'].copy()\n",
    "test = traintest[traintest['set']=='test'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recompute the same metadata information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(np.unique(traintest[user_field].values))\n",
    "items = list(np.unique(traintest[item_field].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_metadata = traintest.drop_duplicates(subset=['item_id'], keep='first')\n",
    "category_per_item = items_metadata[type_field].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to import and usethe re-ranking model mentioned above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ranker_xquad import RankerXQuad\n",
    "model = RankerXQuad(users, items, train, test, category_per_item, item_field, user_field, rating_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model_type = 'utime_pairwise' # This string identifies the recommendation algorithm where the re-ranking is applied\n",
    "reranked_model_type = original_model_type + '_' + 'xquad' # This string is an identifier for models results in data/outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the model predictions with those of the original recommendation model, precomputed in the first notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = load_obj(os.path.join(data_path,'outputs/predictions/' + dataset + '_' + original_model_type + '_scores.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_predictions(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run the re-ranking process and save the positional relevance of items for users after re-ranking (**around 30 mins**).\n",
    "\n",
    "- **lmbda** is the weight given to the popularity regularization.\n",
    "- **k** indicates the length of the recommended list to regularize.\n",
    "- **rmax** indicates the neighborhood to look for while selecting less popular items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rerank(type='smooth', lmbda=0.4, k=10, rmax=100, head_tail_split=head_tail_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.get_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(predictions, os.path.join(data_path, 'outputs/predictions/' + dataset + '_' + reranked_model_type + '_scores.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute and show the metrics for the recommender systems obtained after re-ranking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test(item_group=item_group, cutoffs=cutoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(metrics, os.path.join(data_path, 'outputs/metrics/' + dataset + '_' + reranked_model_type + '_metrics.pkl'))\n",
    "'saved in ' +os.path.join(data_path, 'outputs/metrics/' + dataset + '_' + reranked_model_type + '_metrics.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Speed up the process]\n",
    "\n",
    "This re-ranking takes several minutes. For this tutorial, please feel free to stop and load directly our pre-computed re-ranking predictions (2 MB).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyDrive\n",
    "\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "downloaded = drive.CreateFile({'id':'1lAmSPd19F4WRfWz0TOdFcOzHUMieSJOX'}) \n",
    "downloaded.GetContentFile('../data/outputs/metrics/ml1m_utime_pairwise_xquad_metrics.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_metrics(load_obj('../data/outputs/metrics/ml1m_utime_pairwise_xquad_metrics.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.show_metrics(index_k=int(np.where(cutoffs == 10)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have obtained the metrics resulting from the considered strategy. Now, we come back to compare the results obtained with these strategy against the ones of the baseline recommendation algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = ['utime_pointwise', 'utime_pairwise', 'utime_pairwise_xquad', 'utime_random', 'utime_mostpop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = load_metrics(model_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_effectiveness(metrics, np.array([5, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_item_related_metrics(metrics, np.array([5, 10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6:  Compare the impact of two different mitigation procedures. \n",
    "\n",
    "Now, we compate the in- and post-processing mitigation results, with the baseline pairwise recommender list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = ['utime_pairwise', 'utime_1_pairwise', 'utime_5_pairwise', 'utime_pairwise_xquad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = load_metrics(model_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_effectiveness(metrics, np.array([5, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_item_related_metrics(metrics, np.array([5, 10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we have analyzed and mitigated biases against item popularity. Specifically, we have loaded the working environment from GDrive and the Movielens 1M dataset, inspected the recommended lists based on item popularity, and defined, run, and inspected the impact of an in-processing and a post-processing mitigation procedure. \n",
    "\n",
    "## Further Steps\n",
    "- Run the mitigation approaches with other lmbda values and go deeply into the impact of lmbda in the recommended lists. \n",
    "- Prepare another recommendation dataset and apply the above analyses to a new domain (you just need to properly adjust the csv  of the new dataset as thatof Movielens 1M and change the path to the dataset used in our notebooks). \n",
    "- Create a copy of the model you want to consider for your strategy (e.g., models/pairwise.py) and add your mitigation strategy (you can start by looking to the ./models/pairwise_reg.py and understangin how we have prepared the in-processing mitigation) \n",
    "- Create a copy of the re-ranker mentioned above (i.e., models/ranker_xquad) and used it as a basis for another re-ranking strategy you want to apply.   \n",
    "\n",
    "## Suggested Reading\n",
    "\n",
    "If you are interested in a full showcase of how to implement the proposed in-processing mitigation procedure, you could read:\n",
    "\n",
    "**Boratto, Ludovico, Gianni Fenu, and Mirko Marras**. Connecting user and item perspectives in popularity debiasing for collaborative recommendation, Information Processing & Management, Volume 58, Issue 1, 2021, 102387, ISSN 0306-4573. [Elsevier Link](https://doi.org/10.1016/j.ipm.2020.102387)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
