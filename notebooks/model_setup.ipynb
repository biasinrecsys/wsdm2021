{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Notebook #1: Designing and evaluating a recommendation algorithm\n",
    "\n",
    "In this notebook, we will focus on becoming familiar with the recommendation pipeline through an introductory Python toolbox, in the simplest possible way. Specifically, we will:\n",
    "\n",
    "**Step 1** Setup the working environment in GDrive. \n",
    "\n",
    "**Step 2** Load and understand the Movielens 1M dataset.\n",
    "\n",
    "**Step 3** Split data in training and test sets.\n",
    "\n",
    "**Step 4** Define a pointwise / pairwise / random / mostpop recommendation algorithm.\n",
    "\n",
    "**Step 5** Train a recommendation model (only for point-wise and pair-wise).\n",
    "\n",
    "**Step 6** Compute the user-item matrix that includes the predicted relevance scores.\n",
    "\n",
    "**Step 7** Calculate evaluation metrics to monitor properties like effectiveness, catalog coverage, and novelty.  \n",
    "\n",
    "**Step 8** Run the full pipeline for the other algorithms under consideration.   \n",
    "\n",
    "For each step of the pipeline, we will save the corresponding computations (e.g., pre-trained models, user-item relevance matrices and so on). These artifacts will be the starting point of the investigation covered in the subsequent notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup the working environment in GDrive. \n",
    "\n",
    "Requirements for your working environment:\n",
    "\n",
    "- Python >= 3.6\n",
    "- Package Requirements: pandas, numpy, scipy, matplotlib, scikit-learn, tensorflow. \n",
    "- GDrive storage requirements: ~1GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount the GDrive storage\n",
    "\n",
    "This step serves to mount GDrive storage within this Jupyter notebook. The command will request us to give access permissions to this notebook, so that we will be able to clone the project repository when we desire. Please follow the prompted instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will clone the project repository in our My Drive folder. If you wish to change the target folder, please modify the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/gdrive/My Drive/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone the Github repository into GDrive\n",
    "\n",
    "If you want to work with the codebase locally in your laptop, you should start to run the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/biasinrecsys/wsdm2021.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will move to the project folder in order to install the required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd wsdm2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will configure the notebooks directory as our working directory in order to simulate a local notebook execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.train_test_splitter import *\n",
    "from models.pointwise import PointWise\n",
    "from models.pairwise import PairWise\n",
    "from models.mostpop import MostPop\n",
    "from models.random import Random\n",
    "from helpers.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create folders for saving pre-computed results\n",
    "\n",
    "We will define the subfolders in **./data** where we will store our pre-computed results. For each dataset:\n",
    "\n",
    "- *data/outputs/splits* will include two csv files including the train and test interactions, according with the selected train-test split rule. \n",
    "- *data/outputs/instances* will include a csv file with instances to be fed to the model, either pairs for point-wise or triplets for pair-wise recommenders.\n",
    "- *data/outputs/models* will include a h5 file associated with a pre-trained recommender model.  \n",
    "- *data/outputs/predictions* will include a numpy file representing a user-item matrix; a cell stores the relevance score of an item for a given user.\n",
    "- *data/outputs/metrics* will include a pickle dictionary with the computed evaluation metrics for a given recommender model. \n",
    "\n",
    "**N.B.** This strategy will allow us to play with the intermediate outputs of the pipeline, without starting from scratch any time (e.g., for performing a bias treatment as a post-processing, we just need to load the predictions of a model to start). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir '../data/outputs'\n",
    "!mkdir '../data/outputs/splits'\n",
    "!mkdir '../data/outputs/instances'\n",
    "!mkdir '../data/outputs/models'\n",
    "!mkdir '../data/outputs/predictions'\n",
    "!mkdir '../data/outputs/metrics'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and understand the Movielens 1M dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will load the **Movielens 1M** dataset, which has been pre-arranged in order to comply with the following structure:\n",
    "\n",
    "- user_id\n",
    "- item_id\n",
    "- rating\n",
    "- timestamp\n",
    "- type (label for the item category\n",
    "- type_id (unique id of the item category)\n",
    "\n",
    "For the sake of tutorial easiness, we assume here that each item is randomly assigned to one of its categories in the original dataset. \n",
    "\n",
    "**N.B.** This toolbox is flexible enough to integrate any other dataset in csv format that has the same structure of the pre-arranged csv shown below. No further changes are then needed to the pipeline in order to experiment with other datasets. The csv file of the new dataset should be placed into the *data/datasets/* folder and the name of the file should be assigned to the *dataset* parameter below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input of this step: CSV file including user preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'ml1m'  \n",
    "user_field = 'user_id'\n",
    "item_field = 'item_id'\n",
    "rating_field = 'rating'\n",
    "time_field = 'timestamp'\n",
    "type_field = 'type_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(data_path, 'datasets/' + dataset + '.csv'), encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(n=10, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short exercise 1: find the id of the most popular item (i.e, the item with the highest number of ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE CELL ### Please, add your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this tutorial, we will simulate a scenario with **implicit feedback**. We assume that a user is interested in an item, if that item was rated by the user, no matter of the rating value. Other strategies can be easily integrated. \n",
    "\n",
    "**N.B.** Other papers in the literature assumed that an item is relevant for a user, only if the user has given a rating higher than a value X. To implement this strategy here, you just need to change the body of the lambda function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[rating_field] = data[rating_field].apply(lambda x: 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Output of this step: Dataframe / CSV file including pre-processed user preferences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(n=10, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Split data in training and test sets\n",
    "\n",
    "Once the original dataset has been loaded and the user preferences have been pre-processed, we need to split the whole dataset in two sets: a training set used for optimizing the recommender model and a test set used for evaluating the recommender model. In the literature, a wide range of train-test split strategy exists. This notebook will use a strategy that, for each user, puts the oldest interactions in the training set and the most recent interactions in the test set. The Python toolbox includes also other strategies, such as a random split or a split based on a fixed timestamp (i.e., the most realistic one).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Input of this step: Dataframe / CSV file including pre-processed user preferences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **smode**: 'uftime' for fixed timestamp split, 'utime' for time-based split per user, 'urandom' for random split per user \n",
    "- **train_ratio**: percentage of data to be included in the train set\n",
    "- **min_train**: minimum number of train samples for a user to be included  \n",
    "- **min_test**: minimum number of test samples for a user to be included\n",
    "- **min_time**: start timestamp for computing the splitting timestamp (only for uftime)\n",
    "- **max_time**: end timestamp for computing the splitting timestamp (only for uftime)\n",
    "- **step_time**: timestamp step for computing the splitting timestamp (only for uftime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smode = 'utime'\n",
    "train_ratio = 0.80        \n",
    "min_train_samples = 8\n",
    "min_test_samples = 2\n",
    "min_time = None\n",
    "max_time = None\n",
    "step_time = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this tutorial, we will work with a common **time-based split per user**. For the sake of clarity, we will provide the implementation of this strategy below. The toolbox conserves all the train-test split strategies into the file *helpers/train_test_splitter.py*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_timestamp(interactions,split=0.80,min_samples=10,user_field='user_id',item_field='item_id',time_field='timestamp'):\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    \n",
    "    groups = interactions.groupby([user_field])\n",
    "    for i, (index, group) in enumerate(groups):\n",
    "        \n",
    "        if len(group.index) < min_samples:\n",
    "            continue\n",
    "        \n",
    "        sorted_group = group.sort_values(time_field)\n",
    "        n_rating_test = int(len(sorted_group.index) * (1.0 - split))\n",
    "        train_set.append(sorted_group.head(len(sorted_group.index) - n_rating_test))\n",
    "        test_set.append(sorted_group.tail(n_rating_test))\n",
    "    \n",
    "    print('\\r> Parsing user', i+1, 'of', len(groups))\n",
    "\n",
    "    train, test = pd.concat(train_set), pd.concat(test_set)\n",
    "    train['set'], test['set'] = 'train', 'test' # Ensure that each row has a column that identifies the associated set\n",
    "\n",
    "    traintest = pd.concat([train, test])\n",
    "    traintest[user_field + '_original'] = traintest[user_field] # Ensure that we save the original user ids\n",
    "    traintest[item_field + '_original'] = traintest[item_field] # Ensure that we save the original item ids\n",
    "    traintest[user_field] = traintest[user_field].astype('category').cat.codes # Ensure that user ids are in [0, |U|] \n",
    "    traintest[item_field] = traintest[item_field].astype('category').cat.codes # Ensure that item ids are in [0, |I|] \n",
    "\n",
    "    return traintest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the training and test set split\n",
    "\n",
    "This notebook can be easily run with any of the different train-test split strategies, through the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if smode == 'uftime':\n",
    "    traintest = fixed_timestamp(data, min_train_samples, min_test_samples, min_time, max_time, step_time, user_field, item_field, time_field, rating_field)\n",
    "elif smode == 'utime':\n",
    "    traintest = user_timestamp(data, train_ratio, min_train_samples+min_test_samples, user_field, item_field, time_field)\n",
    "elif smode == 'urandom':\n",
    "    traintest = user_random(data, train_ratio, min_train_samples+min_test_samples, user_field, item_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N.B.** For the sake of convenience, *user_ids* and *item_ids* have been scaled so that user_ids are in *[0, |U|]* and item_ids are in *[0, |I|]*. To refer back to the original user and item ids, the *user_id_original* and *item_id_original* columns should be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of replicability and efficiency of this tutorial, we will save the pre-computed train and test sets in *data/outputs/splits*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traintest.to_csv(os.path.join(data_path, 'outputs/splits/' + dataset + '_' + smode + '.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Output of this step: Dataframe / CSV file with interactions assigned to training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traintest.sample(n=10, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define a pointwise / pairwise / random / mostpop recommendation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Input of this step: Dataframe / CSV file with interactions assigned to training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = traintest[traintest['set']=='train'].copy()\n",
    "test = traintest[traintest['set']=='test'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short exercise 2: plot the distribution of interactions per item in the training set and in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE CELL ### Please, add your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we show some statistics about the training and test sets, e.g., number of users and items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(np.unique(traintest[user_field].values))\n",
    "items = list(np.unique(traintest[item_field].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(users), len(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that some recommender models may require the category of an item, we create a vector of size *|I|* including the integer-encoded category of the item with id *X* at position *X* of the vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_per_item = traintest.drop_duplicates(subset=['item_id'], keep='first')[type_field].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(category_per_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the recommendation algorithm object\n",
    "\n",
    "For the sake of easiness and time, this tutorial focuses on four main recommendation strategies: \n",
    "\n",
    "**Random**: randomly recommending a list of items to a user. \n",
    "\n",
    "**MostPop**: recommending the same most popular items (i.e, those which received the highest number of ratings) to all users.\n",
    "\n",
    "**PointWise**: given a user-item pair, it is optimized for predicting a higher score (1) when the current item has been rated by the user, and a lower score (0) otherwise. The training instances include a good reprsentation of both types of pairs.   \n",
    "\n",
    "**PairWise**: given a triplet with a user, an observed item, and an unobserved item, it is optimized for predicting a higher relevance for the pair of user and unobserved item rather than for the pair of user and unobserved item. \n",
    "\n",
    "Each model inherits from the Model class defined in *models/model.py* and extends it by overwriting the *train* and *predict* functions of the original model class. This allows us to minimize the reuse of the code. More details on the implementation of the pairwise recommender can be found into *models/pairwise.py*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = {'random': Random, 'mostpop': MostPop, 'pointwise': PointWise, 'pairwise': PairWise}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to initialize the model. We will see how the process works for a PairWise algorithm. Then, we will consider the other ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'pairwise'\n",
    "%time model = PairWise(users, items, train, test, category_per_item, item_field, user_field, rating_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train a recommendation model (only for point-wise and pair-wise).\n",
    "\n",
    "We will train the model by feeding the train data we previously prepared, using the following default parameters. \n",
    "\n",
    "- **no_epochs** (default 100): maximum number of epochs until which the training process will be run. \n",
    "- **batches** (default 1024): size of the batches fed into the model during training. \n",
    "- **lr** (default 0.001): learning rate defining the pace at which the model will be trained. \n",
    "- **no_factors** (default 10): size of the latent vectors associated to users and items. \n",
    "- **no_negatives** (default 10): number of triplets for each user-item pair included in the training set. \n",
    "- **val_split** (default 0.0001): proportion of the training set used for validation. \n",
    "\n",
    "**N.B.** For the sake of tutorial efficiency, we force to stop the training process after 5 epochs (i.e., reasonable trade-off). No grid search on the recommender model is performed at this stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time model.train(no_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of the trained model looks as follows. Essentially, the model includes:\n",
    "- **UserEmb** encoding a latent vector for each user.\n",
    "- **ItemEmb** encoding a latent vector for each item.\n",
    "- **FlatUserEmb** represents the vector associated with the current user *UserInput*.\n",
    "- **FlatPosItemEmb** represents the vectors associated with the current observed item *PosItemInput*.\n",
    "- **FlatNegItemEmb** represents the vectors associated with the current unobserved item *NegItemInput*.\n",
    "- **Accuracy** computes the margin between (i) the *FlatUserEmb-FlatPosItemEmb* and (ii) the *FlatUserEmb-FlatNegItemEmb* similarity scores.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Output of this step: H5 TensorFlow model pre-trained with the interactions in the training set\n",
    "\n",
    "The model file is saved in *data/outputs/models*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Compute the user-item matrix that includes the predicted relevance scores.\n",
    "\n",
    "Once the recommender model has been trained, we leverage the pre-trained user and item Embedding matrices in order to compute the relevance score predicted for each unseen user-item pair. For all the user-item pairs, the prediction step requires to extract the user and item vector associated to the current user-item pair and, then, compute the similarity between the two - cosine or dot similarity are usually used at this stage.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Input of this stage: H5 Tensorflow model pre-trained with the interactions in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use the pre-trained model to predict the user-item relevance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of easiness, you could directly manipulate the user-item relevance matrix as a numpy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.get_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we can access to the relevance score of the user *120* for the item *320* as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id, item_id = 120, 320\n",
    "scores[user_id, item_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short exercise 3: compute the range of the scores on the whole population of users.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE CELL ### Please, add your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of convenience, we will save the predicted scores. They are often used as an input for re-ranking treatments against bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(scores, os.path.join(data_path, 'outputs/predictions/' + dataset + '_' + smode + '_' + model_type + '_scores.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short exercise 4: retrieve the ids of the 10 items with  the highest relevance score for user 47.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE CELL ### Please, add your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Output of this step: Numpy matrix of size |U|*|I| containing the user-item relevance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Calculate evaluation metrics.\n",
    "\n",
    "Finally, with the user-item relevance scores predicted in the previous step, we can generate the recommendations for each user and, then, compute a set of well-known evaluation metrics for recommender systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Input of this step: Numpy matrix of size |U|*|I| containing the user-item relevance scores and a list of cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = np.array([5, 10, 20, 50, 100, 200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of convenience, for the considered recommender model, we also compute some fairness metrics required for the case studies. The following line of code loads the demographic membership of providers, which will be discussed in detail in Notebook #03.\n",
    "\n",
    "**N.B.** While the gender is by no means a binary construct, to the best of our knowledge no dataset for speaker recognition with non-binary genders exists. What we are considering is a binary feature, as the current publicly available datasets offer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_item_association = pd.read_csv(os.path.join(data_path, 'datasets', 'ml1m-dir-group.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframes includes, for each item, the percentage of providers with gender_1 and gender_2 for that item, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_item_association.sample(n=10, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short exercise 5: compute the percentage of items where providers of group_1 are represented.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE CELL ### Please, add your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_maps = {i:g for i, g in zip(group_item_association['item_id'], group_item_association['group_1'])}\n",
    "item_maps = {i1:i2 for i1, i2 in zip(traintest['item_id'].unique(), traintest['item_id_original'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_group = [(1 if item_maps[i] in group_maps and gender_maps[item_maps[i]] == 0 else 0) for i in range(len(items))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we run the function which computes all the metrics relevant for the subsequent case studies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test(item_group=item_group, cutoffs=cutoffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method has pre-computed a set of metrics and saved the corresponding values in a Python dictionary, as detailed below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values for each metrics have been computed and store for each cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, values in metrics.items():\n",
    "    print(values.shape, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, we can access to the NDCG score for the user *120* at cutoff *10*, with the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id, cutoff_index = 1324, int(np.where(cutoffs == 10)[0])\n",
    "metrics['ndcg'][cutoff_index, user_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short exercise 6: compute catalog coverage (i.e., percentage of items recommended at least once) at top-20.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE CELL ### Please, add your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of convenience, we will save the compted metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(metrics, os.path.join(data_path, 'outputs/metrics/' + dataset + '_' + smode + '_' + model_type + '_metrics.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the aggregated values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.show_metrics(index_k=int(np.where(cutoffs == 10)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Output of this step: Dictionary of evaluation metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' - '.join(list(metrics.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Run the full pipeline for the other algorithms under consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a utility function to run all the above operations jointly for each of the other recommender models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_type, no_epochs=None):\n",
    "    print('Running model', model_type)\n",
    "    # Initialize the model\n",
    "    model = model_types[model_type](users, items, train, test, category_per_item, item_field, user_field, rating_field)\n",
    "    # Train the model\n",
    "    model.train(no_epochs=no_epochs) if no_epochs else model.train() \n",
    "    # Make and save predictions\n",
    "    model.predict()\n",
    "    scores = model.get_predictions()\n",
    "    save_obj(scores, os.path.join(data_path, 'outputs/predictions/' + dataset + '_' + smode + '_' + model_type + '_scores.pkl'))\n",
    "    # Compute and save metrics\n",
    "    model.test(item_group=item_group, cutoffs=cutoffs)\n",
    "    metrics = model.get_metrics()\n",
    "    save_obj(metrics, os.path.join(data_path, 'outputs/metrics/' + dataset + '_' + smode + '_' + model_type + '_metrics.pkl'))\n",
    "    # Show evaluation metrics\n",
    "    print('\\n\\nFinal evaluation metrics:')\n",
    "    model.show_metrics(index_k=int(np.where(cutoffs == 10)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model('random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model('mostpop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model('pointwise', no_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we instantiated recommendation pipelines in the simplest possible way. Specifically, we have setup the working environment in GDrive, loaded and understood the Movielens 1M dataset, split data in training and test sets, defined a pointwise / pairwise / random / mostpop recommendation algorithm, trained a recommendation model (only for point-wise and pair-wise), computed the user-item matrix that includes the predicted relevance scores, calculated evaluation metrics to monitor properties, and run the full pipeline for the other algorithms under consideration.  \n",
    "\n",
    "## Further Steps\n",
    "\n",
    "- Take a look at the helpers/train_test_splitter.py file and how the existing generators have been defined. \n",
    "- Similarly, take a look at the helpers/instances_creator.py file and how the existing generators have been defined. \n",
    "- A new subclass of the Model class in models/model.py could be defined, implementing a 'train' and a 'predict' method. \n",
    "- The 'test' and 'show_metrics' methods of models/model.py could be extended with the computation needed by a new metric. \n",
    "\n",
    "## Suggested Reading \n",
    "\n",
    "If you are interested in an example of how to implement this pipeline for an exploratory analysis of bias, you could read:\n",
    "\n",
    "**Boratto, L., Fenu, G., & Marras, M. (2019, April)**. The effect of algorithmic bias on recommender systems for massive open online courses. In European Conference on Information Retrieval (pp. 457-472). Springer, Cham.\n",
    "[Springer Link](https://link.springer.com/chapter/10.1007/978-3-030-15712-8_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
